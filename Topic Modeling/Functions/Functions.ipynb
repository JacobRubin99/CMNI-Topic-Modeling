{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41ce16b-e28d-45b0-afab-68e9456423fd",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6ab3b-de18-4962-b49f-fb98d93a9cba",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05661820-6148-4b6f-9c85-e6e193788a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5177a1d-c56d-41da-8a0f-415bc56414fd",
   "metadata": {},
   "source": [
    "## Customize BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb49c590-d6f3-47a1-97a9-946ca3572292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "def run_custom_BERTopic(docs=[], \n",
    "                        embedding_model=SentenceTransformer(\"all-MiniLM-L6-v2\"), \n",
    "                        vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=list(stopwords.words('english'))),\n",
    "                        min_cluster_size=10, min_samples=10,\n",
    "                       ):\n",
    "\n",
    "    # n_neighbors identifies the kth nearest neighbors (Default is 15, recommended between 3 and 5)\n",
    "    # n_components represents the reduced dimension space we embed the data into (Default is 2)\n",
    "    # min_dist controls the minimum distance points are allowed to be in the final low dimensional representation (Default set to 0.1)\n",
    "    \n",
    "    #setting a random state allows us to fully reproduce the results each time we run the model. prevents stochastic behavior. (https://umap-learn.readthedocs.io/en/latest/reproducibility.html)\n",
    "    umap_model = UMAP(n_neighbors=3, n_components=2, min_dist=0.1, random_state=42)\n",
    "    \n",
    "    # min_cluster_size is the minimum # of points required for a cluster. It is set to 5/10 by default, and has a min. value of 2.\n",
    "    # min_samples is the minimum # of points required to form a core within a cluster. It is set to min_cluster_size by default, and has a min. value of 1.\n",
    "    #      Core points are data points that have at least min_samples neighbors within a specified radius.\n",
    "    #      In other words, min_samples influences how densely points must be distributed within a cluster\n",
    "    \n",
    "    # Summary: min_cluster_size filters out small clusters based on the number of samples they contain, while min_samples controls the density of clusters by specifying the minimum number of neighbors required for a point to be considered a core point. \n",
    "    \n",
    "    # allow_single_cluster is set to true to see if a single overriding cluster exists.\n",
    "    # The gen_min_span_tree and prediction_data params are required for integrating with BERTopic and visualizing clusters later on.\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples,\n",
    "                            allow_single_cluster=True,\n",
    "                            gen_min_span_tree=True,\n",
    "                            prediction_data=True)\n",
    "    \n",
    "    \n",
    "    model = BERTopic(\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        top_n_words=5,\n",
    "        language='english',\n",
    "        calculate_probabilities=True,\n",
    "        # verbose=True , uncomment to see progress as the model runs\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a1766-5ada-466d-8538-2f9196b00f58",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178147e7-5b8d-4d7e-b627-f13cc0245df8",
   "metadata": {},
   "source": [
    "Gathers the following 5 evaluation metrics:\n",
    "\n",
    "(1) **Coherence (Normalized Pointwise Mutual Information - NPMI)**: (how semantically similar the top words of a topic are)\n",
    "\n",
    "The coherence score of a topic is the average NPMI score for all pairs of words within that topic. The NPMI score for a given pair of words is the joint probability that the two words appear together in the documents divided by the individual probabilities that each word appears in the documents.\n",
    "\n",
    "$$PMI = log\\frac{P(w_{i}, w_{j})}{P(w_{i})P(w_{j})}$$\n",
    "$$NPMI = \\frac{PMI(w_{i}, w_{j})}{-log P(w_{i}, w_{j})}$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The normalization adjusts this measurement to give a score between -1 and 1. Higher scores mean the words are more likely to appear together, indicating a more coherent topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6dcca0a8-9259-4174-b5c5-cfea2eaf4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence - NPMI\n",
    "# coherence score of < 0 is very low, indicative that the words across the topics are similar semantically (but this makes sense since all from the same survey?) \n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def coherence_score(docs, model, vectorizer_model):\n",
    "    topics = model.get_topics()\n",
    "\n",
    "    # EXAMPLE ONE WITH DATA PROCESSING\n",
    "    cleaned_docs = model._preprocess_text(docs)\n",
    "    analyzer = vectorizer_model.build_analyzer()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "    topic_words = [\n",
    "       [word for word, probs in model.get_topic(topic) if word != \"\"] for topic in topics\n",
    "    ]\n",
    "    topic_words = [[words for words, _ in model.get_topic(topic)] \n",
    "                for topic in range(len(set(topics))-1)]\n",
    "    \n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "    \n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77f77b-053d-43e7-81af-0ff20a2a4b37",
   "metadata": {},
   "source": [
    "(2) **Sihouette Score**:\n",
    "\n",
    "The silhouette score measures how similar a word is to its own topic compared to other topics. Given the sentence embeddings for each document in a BERT model, you can calculate:\n",
    "\n",
    "$$a(i) \\text{: the avg. distance from one document to all other documents in its assigned topic}$$\n",
    "$$b(i) \\text{: the distance from one document to the nearest other topic}$$\n",
    "\n",
    "$$score = \\frac{b(i) - a(i)}{max(a(i),b(i)}$$\n",
    "\n",
    "Interpretation: \n",
    "\n",
    "Scores range from -1 to 1. A score close to 1 means the object is well-matched to its own cluster and poorly matched to neighboring clusters. If most documents have high silhouette scores (close to 1), it means they are well-grouped into distinct topics. If scores are close to -1, the document is misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fa4c46d-f2d7-4d51-875d-6260092334c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "# Measures how similar an object is to its own cluster compared to other clusters.\n",
    "# Ranges from -1 to 1, where higher values indicate better-defined clusters.\n",
    "# A score of 1 indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def silhouette_metric(docs, embedding_model, model, topics):\n",
    "    embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "    \n",
    "    # Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
    "    umap_embeddings = model.umap_model.transform(embeddings)\n",
    "    indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "    X = umap_embeddings[np.array(indices)]\n",
    "    labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    return silhouette_score(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e69b36-c6ba-456b-a72a-b6256f898c17",
   "metadata": {},
   "source": [
    "(3) **Adjusted Rand Index (ARI)**: (similarity of clusterings between true and predicted topics)\n",
    "\n",
    "\n",
    "Given the set of true_labels (e.g. determined by factor analysis) and predicted_labels (determined by BERTopic), we can calculate how similar the topic/factor assignments are. First, we derive a confusion matrix (true positives, true negatives, ...). Then we run the following:\n",
    "\n",
    "$$RI = \\frac{TP + TN}{TP + TN + FP + FN}$$ \n",
    "Adjusted Rand Index further corrects for chance.\n",
    "\n",
    "Interpretation: \n",
    "\n",
    "A score of 1 indicates perfect agreement between the two labelings, while a score of 0 indicates random labeling, and a score less than 0 indicates disagreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17966111-dd20-4445-8f34-e4bc40716eb4",
   "metadata": {},
   "source": [
    "(4) **Purity**: (quality: how well each topic contains documents from a single true topic)\n",
    "\n",
    "Purity measures the extent to which the documents are exclusively assigned to a single topic.\n",
    "Given the set of true_labels (e.g. determined by factor analysis) and predicted_labels (determined by BERTopic), we can calculate for each predicted topic the proportion of the topic's documents belonging to the most frequent true label (topic) in that cluster.\n",
    "\n",
    "For each topic with n documents:\n",
    "$$ purity = \\frac{\\text{number of documents that belong to most frequent true label topic}}{n}  $$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "A purity score of 1 indicates that all documents in the cluster belong to the same true class, making the cluster perfectly homogeneous.\n",
    "Lower purity scores indicate that the cluster contains a mix of different true classes, reducing its homogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22a2d7-8f22-4c9a-a4f0-fc4d1c48525b",
   "metadata": {},
   "source": [
    "(5) **Normalized Mutual Information (NMI)**: (interdependence between true and predicted labels)\n",
    "\n",
    "TBD\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "A normalized mutual information of 1 indicates perfect agreement between the clusterings, while a score of 0 indicates no agreement beyond chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4829eda2-d4d9-469d-8ea8-55b8358d7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Purity\n",
    "def purity_score(y_true, y_pred):\n",
    "\n",
    "    confusion_matrix = np.zeros((np.max(y_true) + 1, np.max(y_pred) + 1), dtype=int)\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        confusion_matrix[true_label][pred_label] += 1\n",
    "    \n",
    "    return np.sum(np.amax(confusion_matrix, axis=0)) / np.sum(confusion_matrix)\n",
    "\n",
    "\n",
    "# Pass in two lists: true_factors and predicted_factors\n",
    "def evaluation_metrics(true_values, predicted_values):\n",
    "    # Convert categorical data to numerical labels\n",
    "    le_true = LabelEncoder()\n",
    "    le_pred = LabelEncoder()\n",
    "\n",
    "    true_labels = le_true.fit_transform(true_values)\n",
    "    predicted_labels = le_pred.fit_transform(predicted_values)\n",
    "\n",
    "    # Adjusted Rand Index\n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Purity\n",
    "    purity = purity_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Normalized Mutual Information\n",
    "    nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return ari, purity, nmi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
