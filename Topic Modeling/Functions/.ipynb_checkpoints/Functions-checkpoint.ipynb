{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41ce16b-e28d-45b0-afab-68e9456423fd",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6ab3b-de18-4962-b49f-fb98d93a9cba",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05661820-6148-4b6f-9c85-e6e193788a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5177a1d-c56d-41da-8a0f-415bc56414fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Customize BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb49c590-d6f3-47a1-97a9-946ca3572292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "def run_custom_BERTopic(docs=[], \n",
    "                        embedding_model=SentenceTransformer(\"all-MiniLM-L6-v2\"), \n",
    "                        vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=list(stopwords.words('english'))),\n",
    "                        min_cluster_size=10, min_samples=10,\n",
    "                       ):\n",
    "\n",
    "    # n_neighbors identifies the kth nearest neighbors (Default is 15, recommended between 3 and 5)\n",
    "    # n_components represents the reduced dimension space we embed the data into (Default is 2)\n",
    "    # min_dist controls the minimum distance points are allowed to be in the final low dimensional representation (Default set to 0.1)\n",
    "    \n",
    "    #setting a random state allows us to fully reproduce the results each time we run the model. prevents stochastic behavior. (https://umap-learn.readthedocs.io/en/latest/reproducibility.html)\n",
    "    umap_model = UMAP(n_neighbors=3, n_components=2, min_dist=0.1, random_state=42)\n",
    "    \n",
    "    # min_cluster_size is the minimum # of points required for a cluster. It is set to 5/10 by default, and has a min. value of 2.\n",
    "    # min_samples is the minimum # of points required to form a core within a cluster. It is set to min_cluster_size by default, and has a min. value of 1.\n",
    "    #      Core points are data points that have at least min_samples neighbors within a specified radius.\n",
    "    #      In other words, min_samples influences how densely points must be distributed within a cluster\n",
    "    \n",
    "    # Summary: min_cluster_size filters out small clusters based on the number of samples they contain, while min_samples controls the density of clusters by specifying the minimum number of neighbors required for a point to be considered a core point. \n",
    "    \n",
    "    # allow_single_cluster is set to true to see if a single overriding cluster exists.\n",
    "    # The gen_min_span_tree and prediction_data params are required for integrating with BERTopic and visualizing clusters later on.\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples,\n",
    "                            allow_single_cluster=True,\n",
    "                            gen_min_span_tree=True,\n",
    "                            prediction_data=True)\n",
    "    \n",
    "    \n",
    "    model = BERTopic(\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        top_n_words=5,\n",
    "        language='english',\n",
    "        calculate_probabilities=True,\n",
    "        # verbose=True , uncomment to see progress as the model runs\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a1766-5ada-466d-8538-2f9196b00f58",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178147e7-5b8d-4d7e-b627-f13cc0245df8",
   "metadata": {},
   "source": [
    "Gathers the following 5 evaluation metrics:\n",
    "\n",
    "(1) Coherenece (NPMI):\n",
    "\n",
    "(2) Sihouette Score:\n",
    "\n",
    "(3) Adjusted Rand Index:\n",
    "\n",
    "(4) Purity:\n",
    "\n",
    "(5) Normalized Mutual Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6dcca0a8-9259-4174-b5c5-cfea2eaf4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence - NPMI\n",
    "# coherence score of < 0 is very low, indicative that the words across the topics are similar semantically (but this makes sense since all from the same survey?) \n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def coherence_score(docs, model, vectorizer_model):\n",
    "    topics = model.get_topics()\n",
    "\n",
    "    # EXAMPLE ONE WITH DATA PROCESSING\n",
    "    cleaned_docs = model._preprocess_text(docs)\n",
    "    analyzer = vectorizer_model.build_analyzer()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "    topic_words = [\n",
    "       [word for word, probs in model.get_topic(topic) if word != \"\"] for topic in topics\n",
    "    ]\n",
    "    topic_words = [[words for words, _ in model.get_topic(topic)] \n",
    "                for topic in range(len(set(topics))-1)]\n",
    "    \n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "    \n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "    # EXAMPLE TWO WITHOUT DATA PROCESSING\n",
    "    # topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]\n",
    "    \n",
    "    # # Create a Gensim dictionary and corpus from the documents\n",
    "    # texts = [doc.split() for doc in docs]\n",
    "    # dictionary = Dictionary(texts)\n",
    "    # corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # # Create the CoherenceModel using c_npmi\n",
    "    # coherence_model = CoherenceModel(topics=topic_words,\n",
    "    #                                  texts=texts,\n",
    "    #                                  dictionary=dictionary,\n",
    "    #                                  coherence='c_npmi')\n",
    "    # return coherence_model.get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fa4c46d-f2d7-4d51-875d-6260092334c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "# Measures how similar an object is to its own cluster compared to other clusters.\n",
    "# Ranges from -1 to 1, where higher values indicate better-defined clusters.\n",
    "# A score of 1 indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def silhouette_metric(docs, embedding_model, model, topics):\n",
    "    embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "    \n",
    "    # Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
    "    umap_embeddings = model.umap_model.transform(embeddings)\n",
    "    indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "    X = umap_embeddings[np.array(indices)]\n",
    "    labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    return silhouette_score(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4829eda2-d4d9-469d-8ea8-55b8358d7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Purity\n",
    "def purity_score(y_true, y_pred):\n",
    "    # Create contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = np.zeros((np.max(y_true) + 1, np.max(y_pred) + 1), dtype=int)\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        contingency_matrix[true_label][pred_label] += 1\n",
    "    \n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "\n",
    "# Pass in two lists: true_factors and predicted_factors\n",
    "def evaluation_metrics(true_values, predicted_values):\n",
    "    # Convert categorical data to numerical labels\n",
    "    le_true = LabelEncoder()\n",
    "    le_pred = LabelEncoder()\n",
    "\n",
    "    true_labels = le_true.fit_transform(true_values)\n",
    "    predicted_labels = le_pred.fit_transform(predicted_values)\n",
    "\n",
    "    # Adjusted Rand Index (TP + TN) / (TP + FP + TN + FN)\n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Purity\n",
    "    purity = purity_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Normalized Mutual Information\n",
    "    nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return ari, purity, nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426ec94-bdbe-475e-a12e-8adc1fe0f90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
